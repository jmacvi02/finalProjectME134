Link to Demo https://youtu.be/5EHPGYR8pms

**1. Problem**
There are numerous considerations when choosing a control method for a robot. Time for implementation, behavior and design specifications, and personal preference/knowledge all play a part in this process. Balancing all of these factors can be difficult and is a problem that roboticists regularly face. 

With this problem in mind, the purpose of this project was to make a meaningful comparison between a variety of different control methods: remote control of the XRP, classical autonomous control of the XRP, and ML control of the XRP. In comparing these control methods, we sought to better understand the benefits and downsides of each method so we could be more informed when selecting controllers in the future. 

**2. Approach**
To make a comparison between these methods, we first chose a task for the XRP to complete. We thought a challenging task with a clear definition of success was optimal for this project since a task of this nature makes it easier to compare the control methods. We decided to have each XRP race around a rectangular track. Each XRP will have to complete 5 laps around the track. The average lap time and lap time variation were chosen as metrics to compare the methods. Additionally, we intended to reflect on the implementation processes of each as an extra qualitative comparison. 

To achieve remote control, we used an Xbox controller and a laptop. At first, we established a BLE connection between the Xbox controller and the laptop, which was simultaneously running an MQTT server that broadcasted motor commands. The XRP was subscribed to the MQTT channel and listened for these commands. However, this workflow was more complicated than necessary, and it led to moderate to significant latency, oftentimes exceeding 3 seconds. We were able to get Professor Nemitz's help, which allowed us to switch our protocol and ultimately increase our efficiency. After being helped, we optimized the final workflow and cut our latency down. The Xbox controller and the XRP are connected to the laptop via BLE, with the laptop acting as the central hub for smoother command passage. We first mapped the x and y positions of the Xbox controller to a magnitude and angle. Then we mapped this magnitude and angle to motor efforts, which we sent to the XRP. 

To achieve classic autonomous control, we implemented an offset line following algorithm. We used a trained Husky Lens Camera to detect a line along the top of the inner wall of the track. We calculated the slope and average y-axis position of the line, and used these calculations to implement a PD controller with a target slope of 0 and y position of 80 (out of 240). To account for the XRP losing the inner wall, we implemented a simple state machine with the main states of following and searching. The following state corresponds to the PD controller outlined above. The searching state commands the XRP to turn left until the inner wall is found. 

To achieve ML control, we began by looking at larger libraries, such as Scikit or PyTorch. We figured that, like any other Python library, there would be a learning curve, but we were confident that we could put together a good ML script. Since these larger ML libraries cannot run on a lightweight microcontroller like the Pico, we initially planned to create a .pkl file and use an MQTT server to read motor measurements from the laptop to the XRP. However, upon further analysis, we realized that this was likely to be inefficient, since with MQTT comes latency. Therefore, we opted to extract the normalization data and program the math directly onto the Pico. This ensured that we would get the full benefits of the ML model while eliminating any sort of delay in the flow of data. 

**3. Results**
To compare our three car track models, we ran trials where each model would complete five rounds of five laps. Remote-controlled driving was the fastest and most efficient by a large margin; over our trials, RC driving averaged 9.29 seconds while our classical line-following control averaged 24.66 seconds, and our ML model could not even finish a single lap at all (more on that will be covered in the paragraph below). In terms of consistency, however, the classical control had a standard deviation of +/- 1.01 seconds, while the RC model's was +/- 2.23 seconds. Because the line-following algorithm makes the same decision every lap, it lessens the variation in its followed path, whereas the RC robot was driven by a human, which automatically introduces a minimum error.

Before we tested our ML model, we predicted that we hadn't given the model enough data to train on. We ended up being correct because, unfortunately, our robot could not go around the track. However, we were very excited that we were able to do a machine learning project, so it was a great way to end our robotics class. We initially had tried to aim for at least a few thousand data points, but once we began researching more, we discovered that most functional models are trained with millions of data points. We were still optimistic going into it, and we were able to see our model adjust to live sensor readings. For example, when we covered the ultrasonic sensor with our hand, we could see the motor efforts shifting in real time with our hand motions. We may also have been overfitting our data and accounting too much for certain readings under the hood of the ML algorithm. We would have loved to do more data analysis and cleaning; however, this was unfortunately outside the time constraints of our project.

**4. Impact**
We have numerous takeaways from completing this project.

1. Remote control outperforms classic control for one-time tasks: 
Using remote control, the XRP can complete laps significantly faster than the classic controller. This is because the classic control method we used does not adapt its base velocity depending on position on the track, whereas a human can do this manually via remote control. Thus, the XRP can go very fast on straightaways and take tight turns with remote control, whereas the classic controlled XRP has to move slower to avoid collisions with the wall. 

2. Human error can lead to inconsistent results with the remote control application:
As shown in our video, it is difficult to control the XRP with the Xbox controller without actively following behind the XRP. The perspective from a fixed position makes it much harder to determine what inputs to give the Xbox controller. This is because the turning behavior of our remote control algorithm makes relative turns based on the joystick and not absolute turns. This difficulty leads to inconsistent robot behavior during the laps. On the contrary, the classic controller has the same decision-making criterion regardless of where it is with respect to the lap. This consistency is an advantage of classic control. 

Something we did not test was the fatigue of a human user driving around the track. There are likely additional benefits with classic control over remote control in such conditions. 

3. Without data cleaning and feature engineering, ML models are effectively useless:
While we did take precautions on what data we gave our model (e.g., median filter of the distance sensor), there is still clearly some invalid data within our sets. Occasional faulty encoder reads could easily be skewing our model. Additionally, we did not experiment with what data we gave our model. In hindsight, encoder counts alone do not seem to be a relevant datapoint for the model to make predictions on efforts. The efforts produced by the model are almost always outside the -1 to 1 range that the motor requires. It is clear that this model is not intelligently calculating motor efforts to navigate the track. 

Surprisingly, the model does seem to have caught on that the average velocity of the outer wheel is larger than the inner wheel. The right wheel efforts are always much larger than the left wheel efforts. We take this as a sign that creating a model to complete the track is possible in the future with a better emphasis on ML practices. Regardless of the success of our ML, we learned a lot about the process and will be able to apply this knowledge in the future. 

